# Lite.AI.ToolKit.Hub.TNN

You can download all the pretrained models files of TNN format from ([Baidu Drive](https://pan.baidu.com/s/1lvM2YKyUbEc5HKVtqITpcw) code: 6o6k)

## Object Detection.

<div id="lite.ai.toolkit.hub.tnn-object-detection"></div>

|                 Class                 |      Pretrained TNN Files      |              Rename or Converted From (Repo)              | Size  |
| :-----------------------------------: | :-----------------------------: | :-------------------------------------------------------: | :---: |
|     *lite::tnn::cv::detection::YoloV5*     |          yolov5l.opt.tnnproto&tnnmodel            |      [yolov5](https://github.com/ultralytics/yolov5)      | 188Mb |
|     *lite::tnn::cv::detection::YoloV5*     |          yolov5m.opt.tnnproto&tnnmodel            |      [yolov5](https://github.com/ultralytics/yolov5)      | 85Mb  |
|     *lite::tnn::cv::detection::YoloV5*     |          yolov5s.opt.tnnproto&tnnmodel            |      [yolov5](https://github.com/ultralytics/yolov5)      | 29Mb  |
|     *lite::tnn::cv::detection::YoloV5*     |          yolov5x.opt.tnnproto&tnnmodel            |      [yolov5](https://github.com/ultralytics/yolov5)      | 351Mb |  
|     *lite::tnn::cv::detection::YoloX*      |          yolox_x.opt.tnnproto&tnnmodel           |  [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX)   | 378Mb |
|     *lite::tnn::cv::detection::YoloX*      |          yolox_l.opt.tnnproto&tnnmodel           |  [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX)   | 207Mb |
|     *lite::tnn::cv::detection::YoloX*      |          yolox_m.opt.tnnproto&tnnmodel           |  [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX)   | 97Mb  |
|     *lite::tnn::cv::detection::YoloX*      |          yolox_s.opt.tnnproto&tnnmodel           |  [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX)   | 34Mb  |
|     *lite::tnn::cv::detection::YoloX*      |         yolox_tiny.opt.tnnproto&tnnmodel         |  [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX)   | 19Mb  |
|     *lite::tnn::cv::detection::YoloX*      |         yolox_nano.opt.tnnproto&tnnmodel         |  [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX)   | 3.5Mb |
|     *lite::tnn::cv::detection::YOLOP*      |          yolop-320-320.opt.tnnproto&tnnmodel           |  [YOLOP](https://github.com/hustvl/YOLOP)   | 30Mb |
|     *lite::tnn::cv::detection::YOLOP*      |          yolop-640-640.opt.tnnproto&tnnmodel           |  [YOLOP](https://github.com/hustvl/YOLOP)   | 30Mb |
|     *lite::tnn::cv::detection::YOLOP*      |          yolop-1280-1280.opt.tnnproto&tnnmodel           |  [YOLOP](https://github.com/hustvl/YOLOP)   | 30Mb  |
| *lite::tnn::cv::detection::NanoDet* |    nanodet_m_0.5x.opt.tnnproto&tnnmodel     |       [nanodet](https://github.com/RangiLyu/nanodet)       | 1.1Mb  |
| *lite::tnn::cv::detection::NanoDet* |    nanodet_m.opt.tnnproto&tnnmodel     |       [nanodet](https://github.com/RangiLyu/nanodet)       | 3.6Mb  |
| *lite::tnn::cv::detection::NanoDet* |    nanodet_m_1.5x.opt.tnnproto&tnnmodel     |       [nanodet](https://github.com/RangiLyu/nanodet)       | 7.9Mb  |
| *lite::tnn::cv::detection::NanoDet* |    nanodet_m_1.5x_416.opt.tnnproto&tnnmodel     |       [nanodet](https://github.com/RangiLyu/nanodet)       | 7.9Mb  |
| *lite::tnn::cv::detection::NanoDet* |    nanodet_m_416.opt.tnnproto&tnnmodel     |       [nanodet](https://github.com/RangiLyu/nanodet)       | 3.6Mb  |
| *lite::tnn::cv::detection::NanoDet* |    nanodet_g.opt.tnnproto&tnnmodel     |       [nanodet](https://github.com/RangiLyu/nanodet)       | 14Mb  |
| *lite::tnn::cv::detection::NanoDet* |    nanodet_t.opt.tnnproto&tnnmodel     |       [nanodet](https://github.com/RangiLyu/nanodet)       | 5.1Mb  |
| *lite::tnn::cv::detection::NanoDet* |    nanodet-RepVGG-A0_416.opt.tnnproto&tnnmodel     |       [nanodet](https://github.com/RangiLyu/nanodet)       | 26Mb  |
| *lite::tnn::cv::detection::NanoDetEfficientNetLite* |    nanodet-EfficientNet-Lite0_320.opt.tnnproto&tnnmodel     |       [nanodet](https://github.com/RangiLyu/nanodet)       | 12Mb  |
| *lite::tnn::cv::detection::NanoDetEfficientNetLite* |    nanodet-EfficientNet-Lite1_416.opt.tnnproto&tnnmodel     |       [nanodet](https://github.com/RangiLyu/nanodet)       | 15Mb  |
| *lite::tnn::cv::detection::NanoDetEfficientNetLite* |    nanodet-EfficientNet-Lite2_512.opt.tnnproto&tnnmodel     |       [nanodet](https://github.com/RangiLyu/nanodet)       | 18Mb  |
|     *lite::tnn::cv::detection::YoloX_V_0_1_1*      |          yolox_x_v0.1.1.opt.tnnproto&tnnmodel           |  [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX)   | 378Mb |
|     *lite::tnn::cv::detection::YoloX_V_0_1_1*      |          yolox_l_v0.1.1.opt.tnnproto&tnnmodel           |  [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX)   | 207Mb |
|     *lite::tnn::cv::detection::YoloX_V_0_1_1*      |          yolox_m_v0.1.1.opt.tnnproto&tnnmodel           |  [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX)   | 97Mb  |
|     *lite::tnn::cv::detection::YoloX_V_0_1_1*      |          yolox_s_v0.1.1.opt.tnnproto&tnnmodel           |  [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX)   | 34Mb  |
|     *lite::tnn::cv::detection::YoloX_V_0_1_1*      |         yolox_tiny_v0.1.1.opt.tnnproto&tnnmodel         |  [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX)   | 19Mb  |
|     *lite::tnn::cv::detection::YoloX_V_0_1_1*      |         yolox_nano_v0.1.1.opt.tnnproto&tnnmodel         |  [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX)   | 3.5Mb |
|     *lite::tnn::cv::detection::YoloR*     |          yolor-p6-320-320.opt.tnnproto&tnnmodel            |      [yolor](https://github.com/WongKinYiu/yolor)      | 157Mb |
|     *lite::tnn::cv::detection::YoloR*     |          yolor-p6-640-640.opt.tnnproto&tnnmodel            |      [yolor](https://github.com/WongKinYiu/yolor)      | 157Mb  |
|     *lite::tnn::cv::detection::YoloR*     |          yolor-ssss-s2d-640-640.opt.tnnproto&tnnmodel            |      [yolor](https://github.com/WongKinYiu/yolor)      | 50Mb  |
|     *lite::tnn::cv::detection::YoloV5_V_6_0*     |          yolov5m.640-640.v.6.0.opt.tnnproto&tnnmodel       |      [yolov5](https://github.com/ultralytics/yolov5)      | 81Mb  |
|     *lite::tnn::cv::detection::YoloV5_V_6_0*     |          yolov5s.640-640.v.6.0.opt.tnnproto&tnnmodel           |      [yolov5](https://github.com/ultralytics/yolov5)      | 28Mb  |
|     *lite::tnn::cv::detection::YoloV5_V_6_0*     |          yolov5n.640-640.v.6.0.opt.tnnproto&tnnmodel           |      [yolov5](https://github.com/ultralytics/yolov5)      | 7.5Mb |
|     *lite::tnn::cv::detection::YoloV5_V_6_0*     |          yolov5m6.640-640.v.6.0.opt.tnnproto&tnnmodel           |      [yolov5](https://github.com/ultralytics/yolov5)      | 128Mb  |
|     *lite::tnn::cv::detection::YoloV5_V_6_0*     |          yolov5s6.640-640.v.6.0.opt.tnnproto&tnnmodel           |      [yolov5](https://github.com/ultralytics/yolov5)      | 50Mb  |
|     *lite::tnn::cv::detection::YoloV5_V_6_0*     |          yolov5n6.640-640.v.6.0.opt.tnnproto&tnnmodel           |      [yolov5](https://github.com/ultralytics/yolov5)      | 14Mb |
|     *lite::tnn::cv::detection::YoloV5_V_6_0*     |          yolov5m6.1280-1280.v.6.0.opt.tnnproto&tnnmodel           |      [yolov5](https://github.com/ultralytics/yolov5)      | 128Mb  |
|     *lite::tnn::cv::detection::YoloV5_V_6_0*     |          yolov5s6.1280-1280.v.6.0.opt.tnnproto&tnnmodel           |      [yolov5](https://github.com/ultralytics/yolov5)      | 50Mb  |
|     *lite::tnn::cv::detection::YoloV5_V_6_0*     |          yolov5n6.1280-1280.v.6.0.opt.tnnproto&tnnmodel           |      [yolov5](https://github.com/ultralytics/yolov5)      | 14Mb |
| *lite::mnn::cv::detection::NanoDetPlus* |    nanodet-plus-m_320.opt.tnnproto&tnnmodel     |       [nanodet](https://github.com/RangiLyu/nanodet)       | 4.5Mb  |
| *lite::mnn::cv::detection::NanoDetPlus* |    nanodet-plus-m_416.opt.tnnproto&tnnmodel     |       [nanodet](https://github.com/RangiLyu/nanodet)       | 4.5Mb  |
| *lite::mnn::cv::detection::NanoDetPlus* |    nanodet-plus-m-1.5x_320.opt.tnnproto&tnnmodel     |       [nanodet](https://github.com/RangiLyu/nanodet)       | 9.4Mb  |
| *lite::mnn::cv::detection::NanoDetPlus* |    nanodet-plus-m-1.5x_416.opt.tnnproto&tnnmodel     |       [nanodet](https://github.com/RangiLyu/nanodet)       | 9.4Mb  |


## Matting.

<div id="lite.ai.toolkit.hub.tnn-matting"></div>

|                Class                | Pretrained TNN Files |              Rename or Converted From (Repo)              | Size  |
| :---------------------------------: | :-------------------: | :-------------------------------------------------------: | :---: |
| *lite::tnn::cv::matting::RobustVideoMatting* |   rvm_mobilenetv3_fp32-480-480-sim.tnnproto&tnnmodel   | [RobustVideoMatting](https://github.com/PeterL1n/RobustVideoMatting) | 14Mb |
| *lite::tnn::cv::matting::RobustVideoMatting* |   rvm_mobilenetv3_fp32-480-480-sim.opt.tnnproto&tnnmodel   | [RobustVideoMatting](https://github.com/PeterL1n/RobustVideoMatting) | 14Mb |
| *lite::tnn::cv::matting::RobustVideoMatting* |   rvm_mobilenetv3_fp32-480-640-sim.opt.tnnproto&tnnmodel   | [RobustVideoMatting](https://github.com/PeterL1n/RobustVideoMatting) | 14Mb |
| *lite::tnn::cv::matting::RobustVideoMatting* |   rvm_mobilenetv3_fp32-640-480-sim.opt.tnnproto&tnnmodel   | [RobustVideoMatting](https://github.com/PeterL1n/RobustVideoMatting) | 14Mb |
| *lite::tnn::cv::matting::RobustVideoMatting* |   rvm_mobilenetv3_fp32-1080-1920-sim.opt.tnnproto&tnnmodel   | [RobustVideoMatting](https://github.com/PeterL1n/RobustVideoMatting) | 14Mb |
| *lite::tnn::cv::matting::RobustVideoMatting* |   rvm_resnet50_fp32-480-480-sim.opt.tnnproto&tnnmodel   | [RobustVideoMatting](https://github.com/PeterL1n/RobustVideoMatting) | 50Mb |
| *lite::tnn::cv::matting::RobustVideoMatting* |   rvm_resnet50_fp32-480-640-sim.opt.tnnproto&tnnmodel   | [RobustVideoMatting](https://github.com/PeterL1n/RobustVideoMatting) | 50Mb |
| *lite::tnn::cv::matting::RobustVideoMatting* |   rvm_resnet50_fp32-640-480-sim.opt.tnnproto&tnnmodel   | [RobustVideoMatting](https://github.com/PeterL1n/RobustVideoMatting) | 50Mb |
| *lite::tnn::cv::matting::RobustVideoMatting* |   rvm_resnet50_fp32-1080-1920-sim.opt.tnnproto&tnnmodel   | [RobustVideoMatting](https://github.com/PeterL1n/RobustVideoMatting) | 50Mb |
| *lite::tnn::cv::matting::MGMatting* |   MGMatting-DIM-100k.opt.tnnproto&tnnmodel   | [MGMatting](https://github.com/yucornetto/MGMatting) | 113Mb |
| *lite::tnn::cv::matting::MGMatting* |   MGMatting-RWP-100k.opt.tnnproto&tnnmodel   | [MGMatting](https://github.com/yucornetto/MGMatting) | 113Mb |

## Face Recognition.

<div id="lite.ai.toolkit.hub.tnn-face-recognition"></div>  


|                   Class                   |            Pretrained TNN Files             |               Rename or Converted From (Repo)                | Size  |
| :---------------------------------------: | :------------------------------------------: | :----------------------------------------------------------: | :---: |
|     *lite::tnn::cv::faceid::GlintArcFace*      |           ms1mv3_arcface_r100.opt.tnnproto&tnnmodel           |  [insightface](https://github.com/deepinsight/insightface)   | 248Mb |
|     *lite::tnn::cv::faceid::GlintArcFace*      |           ms1mv3_arcface_r50.opt.tnnproto&tnnmodel            |  [insightface](https://github.com/deepinsight/insightface)   | 166Mb |
|     *lite::tnn::cv::faceid::GlintArcFace*      |           ms1mv3_arcface_r34.opt.tnnproto&tnnmodel            |  [insightface](https://github.com/deepinsight/insightface)   | 130Mb |
|     *lite::tnn::cv::faceid::GlintArcFace*      |           ms1mv3_arcface_r18.opt.tnnproto&tnnmodel            |  [insightface](https://github.com/deepinsight/insightface)   | 91Mb  |
|     *lite::tnn::cv::faceid::GlintCosFace*      |         glint360k_cosface_r100.opt.tnnproto&tnnmodel          |  [insightface](https://github.com/deepinsight/insightface)   | 248Mb |
|     *lite::tnn::cv::faceid::GlintCosFace*      |          glint360k_cosface_r50.opt.tnnproto&tnnmodel          |  [insightface](https://github.com/deepinsight/insightface)   | 166Mb |
|     *lite::tnn::cv::faceid::GlintCosFace*      |          glint360k_cosface_r34.opt.tnnproto&tnnmodel          |  [insightface](https://github.com/deepinsight/insightface)   | 130Mb |
|     *lite::tnn::cv::faceid::GlintCosFace*      |          glint360k_cosface_r18.opt.tnnproto&tnnmodel          |  [insightface](https://github.com/deepinsight/insightface)   | 91Mb  |
|    *lite::tnn::cv::faceid::GlintPartialFC*     |        partial_fc_glint360k_r100.opt.tnnproto&tnnmodel        |  [insightface](https://github.com/deepinsight/insightface)   | 248Mb |
|    *lite::tnn::cv::faceid::GlintPartialFC*     |        partial_fc_glint360k_r50.opt.tnnproto&tnnmodel         |  [insightface](https://github.com/deepinsight/insightface)   | 91Mb  |
|        *lite::tnn::cv::faceid::FaceNet*        |         facenet_vggface2_resnet.opt.tnnproto&tnnmodel         |  [facenet...](https://github.com/timesler/facenet-pytorch)   | 89Mb  |
|        *lite::tnn::cv::faceid::FaceNet*        |      facenet_casia-webface_resnet.opt.tnnproto&tnnmodel       |  [facenet...](https://github.com/timesler/facenet-pytorch)   | 89Mb  |
|     *lite::tnn::cv::faceid::FocalArcFace*      |        focal-arcface-ms1m-ir152.opt.tnnproto&tnnmodel         | [face.evoLVe...](https://github.com/ZhaoJ9014/face.evoLVe.PyTorch) | 269Mb |
|     *lite::tnn::cv::faceid::FocalArcFace*      |    focal-arcface-ms1m-ir50-epoch120.opt.tnnproto&tnnmodel     | [face.evoLVe...](https://github.com/ZhaoJ9014/face.evoLVe.PyTorch) | 166Mb |
|     *lite::tnn::cv::faceid::FocalArcFace*      |     focal-arcface-ms1m-ir50-epoch63.opt.tnnproto&tnnmodel     | [face.evoLVe...](https://github.com/ZhaoJ9014/face.evoLVe.PyTorch) | 166Mb |
|   *lite::tnn::cv::faceid::FocalAsiaArcFace*    |       focal-arcface-bh-ir50-asia.opt.tnnproto&tnnmodel        | [face.evoLVe...](https://github.com/ZhaoJ9014/face.evoLVe.PyTorch) | 166Mb |
| *lite::tnn::cv::faceid::TencentCurricularFace* |     Tencent_CurricularFace_Backbone.opt.tnnproto&tnnmodel     |          [TFace](https://github.com/Tencent/TFace)           | 249Mb |
|    *lite::tnn::cv::faceid::TencentCifpFace*    |  Tencent_Cifp_BUPT_Balancedface_IR_34.opt.tnnproto&tnnmodel   |          [TFace](https://github.com/Tencent/TFace)           | 130Mb |
|    *lite::tnn::cv::faceid::CenterLossFace*     |        CenterLossFace_epoch_100.opt.tnnproto&tnnmodel         | [center-loss...](https://github.com/louis-she/center-loss.pytorch) | 280Mb |
|      *lite::tnn::cv::faceid::SphereFace*       |           sphere20a_20171020.opt.tnnproto&tnnmodel            | [sphere...](https://github.com/clcarwin/sphereface_pytorch)  | 86Mb  |
|     *lite::tnn::cv::faceid:MobileFaceNet*      |        MobileFaceNet_Pytorch_068.opt.tnnproto&tnnmodel        | [MobileFace...](https://github.com/Xiaoccer/MobileFaceNet_Pytorch) | 3.8Mb |
|    *lite::tnn::cv::faceid:CavaGhostArcFace*    | cavaface_GhostNet_x1.3_Arcface_Epoch_24.opt.tnnproto&tnnmodel | [cavaface...](https://github.com/cavalleria/cavaface.pytorch) | 15Mb  |
|    *lite::tnn::cv::faceid:CavaCombinedFace*    |  cavaface_IR_SE_100_Combined_Epoch_24.opt.tnnproto&tnnmodel   | [cavaface...](https://github.com/cavalleria/cavaface.pytorch) | 250Mb |
|    *lite::tnn::cv::faceid:MobileSEFocalFace*   | face_recognition.pytorch_Mobilenet_se_focal_121000.opt.tnnproto&tnnmodel | [face_recog...](https://github.com/grib0ed0v/face_recognition.pytorch) | 4.5Mb |

## Face Detection.

<div id="lite.ai.toolkit.hub.tnn-face-detection"></div>  

|                Class                | Pretrained TNN Files  |               Rename or Converted From (Repo)                | Size  |
| :---------------------------------: | :--------------------: | :----------------------------------------------------------: | :---: |
| *lite::tnn::cv::face::detect::UltraFace* | ultraface-rfb-320.opt.tnnproto&tnnmodel | [Ultra-Light...](https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB) | 1.5Mb |
| *lite::tnn::cv::face::detect::UltraFace* | ultraface-rfb-640.opt.tnnproto&tnnmodel | [Ultra-Light...](https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB) | 1.5Mb |
| *lite::tnn::cv::face::detect::UltraFace* | ultraface-slim-320.opt.tnnproto&tnnmodel | [Ultra-Light...](https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB) | 1.2Mb |
| *lite::tnn::cv::face::detect::UltraFace* | ultraface-slim-640.opt.tnnproto&tnnmodel | [Ultra-Light...](https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB) | 1.2Mb |
| *lite::tnn::cv::face::detect::RetinaFace* | Pytorch_RetinaFace_mobile0.25.opt.tnnproto&tnnmodel | [...Retinaface](https://github.com/biubug6/Pytorch_Retinaface) | 1.6Mb |
| *lite::tnn::cv::face::detect::RetinaFace* | Pytorch_RetinaFace_mobile0.25-640-640.opt.tnnproto&tnnmodel | [...Retinaface](https://github.com/biubug6/Pytorch_Retinaface) | 1.6Mb |
| *lite::tnn::cv::face::detect::RetinaFace* | Pytorch_RetinaFace_mobile0.25-320-320.opt.tnnproto&tnnmodel | [...Retinaface](https://github.com/biubug6/Pytorch_Retinaface) | 1.6Mb |
| *lite::tnn::cv::face::detect::RetinaFace* | Pytorch_RetinaFace_mobile0.25-720-1080.opt.tnnproto&tnnmodel | [...Retinaface](https://github.com/biubug6/Pytorch_Retinaface) | 1.6Mb |
| *lite::tnn::cv::face::detect::FaceBoxes* | FaceBoxes.opt.tnnproto&tnnmodel | [FaceBoxes](https://github.com/zisianw/FaceBoxes.PyTorch)  | 3.8Mb |
| *lite::tnn::cv::face::detect::FaceBoxes* | FaceBoxes-640-640.opt.tnnproto&tnnmodel | [FaceBoxes](https://github.com/zisianw/FaceBoxes.PyTorch)  | 3.8Mb |
| *lite::tnn::cv::face::detect::FaceBoxes* | FaceBoxes-320-320.opt.tnnproto&tnnmodel | [FaceBoxes](https://github.com/zisianw/FaceBoxes.PyTorch)  | 3.8Mb |
| *lite::tnn::cv::face::detect::FaceBoxes* | FaceBoxes-720-1080.opt.tnnproto&tnnmodel | [FaceBoxes](https://github.com/zisianw/FaceBoxes.PyTorch)  | 3.8Mb |
| *lite::tnn::cv::face::detect::SCRFD* | scrfd_500m_shape160x160.opt.tnnproto&tnnmodel | [SCRFD](https://github.com/deepinsight/insightface/blob/master/detection/scrfd)  | 2.5Mb |
| *lite::tnn::cv::face::detect::SCRFD* | scrfd_500m_shape320x320.opt.tnnproto&tnnmodel | [SCRFD](https://github.com/deepinsight/insightface/blob/master/detection/scrfd)  | 2.5Mb |
| *lite::tnn::cv::face::detect::SCRFD* | scrfd_500m_shape640x640.opt.tnnproto&tnnmodel | [SCRFD](https://github.com/deepinsight/insightface/blob/master/detection/scrfd)  | 2.5Mb |
| *lite::tnn::cv::face::detect::SCRFD* | scrfd_500m_bnkps_shape160x160.opt.tnnproto&tnnmodel | [SCRFD](https://github.com/deepinsight/insightface/blob/master/detection/scrfd)  | 2.5Mb |  
| *lite::tnn::cv::face::detect::SCRFD* | scrfd_500m_bnkps_shape320x320.opt.tnnproto&tnnmodel | [SCRFD](https://github.com/deepinsight/insightface/blob/master/detection/scrfd)  | 2.5Mb |  
| *lite::tnn::cv::face::detect::SCRFD* | scrfd_500m_bnkps_shape640x640.opt.tnnproto&tnnmodel | [SCRFD](https://github.com/deepinsight/insightface/blob/master/detection/scrfd)  | 2.5Mb |  
| *lite::tnn::cv::face::detect::SCRFD* | scrfd_1g_shape160x160.opt.tnnproto&tnnmodel | [SCRFD](https://github.com/deepinsight/insightface/blob/master/detection/scrfd)  | 2.7Mb |
| *lite::tnn::cv::face::detect::SCRFD* | scrfd_1g_shape320x320.opt.tnnproto&tnnmodel | [SCRFD](https://github.com/deepinsight/insightface/blob/master/detection/scrfd)  | 2.7Mb |
| *lite::tnn::cv::face::detect::SCRFD* | scrfd_1g_shape640x640.opt.tnnproto&tnnmodel | [SCRFD](https://github.com/deepinsight/insightface/blob/master/detection/scrfd)  | 2.7Mb |
| *lite::tnn::cv::face::detect::SCRFD* | scrfd_2.5g_shape160x160.opt.tnnproto&tnnmodel | [SCRFD](https://github.com/deepinsight/insightface/blob/master/detection/scrfd)  | 3.3Mb |
| *lite::tnn::cv::face::detect::SCRFD* | scrfd_2.5g_shape320x320.opt.tnnproto&tnnmodel | [SCRFD](https://github.com/deepinsight/insightface/blob/master/detection/scrfd)  | 3.3Mb |
| *lite::tnn::cv::face::detect::SCRFD* | scrfd_2.5g_shape640x640.opt.tnnproto&tnnmodel | [SCRFD](https://github.com/deepinsight/insightface/blob/master/detection/scrfd)  | 3.3Mb |
| *lite::tnn::cv::face::detect::SCRFD* | scrfd_2.5g_bnkps_shape160x160.opt.tnnproto&tnnmodel | [SCRFD](https://github.com/deepinsight/insightface/blob/master/detection/scrfd)  | 3.3Mb |  
| *lite::tnn::cv::face::detect::SCRFD* | scrfd_2.5g_bnkps_shape320x320.opt.tnnproto&tnnmodel | [SCRFD](https://github.com/deepinsight/insightface/blob/master/detection/scrfd)  | 3.3Mb |  
| *lite::tnn::cv::face::detect::SCRFD* | scrfd_2.5g_bnkps_shape640x640.opt.tnnproto&tnnmodel | [SCRFD](https://github.com/deepinsight/insightface/blob/master/detection/scrfd)  | 3.3Mb |  
| *lite::tnn::cv::face::detect::SCRFD* | scrfd_10g_shape640x640.opt.tnnproto&tnnmodel | [SCRFD](https://github.com/deepinsight/insightface/blob/master/detection/scrfd)  | 16.9Mb |
| *lite::tnn::cv::face::detect::SCRFD* | scrfd_10g_shape1280x1280.opt.tnnproto&tnnmodel | [SCRFD](https://github.com/deepinsight/insightface/blob/master/detection/scrfd)  | 16.9Mb |
| *lite::tnn::cv::face::detect::SCRFD* | scrfd_10g_bnkps_shape640x640.opt.tnnproto&tnnmodel | [SCRFD](https://github.com/deepinsight/insightface/blob/master/detection/scrfd)  | 16.9Mb |  
| *lite::tnn::cv::face::detect::SCRFD* | scrfd_10g_bnkps_shape1280x1280.opt.tnnproto&tnnmodel | [SCRFD](https://github.com/deepinsight/insightface/blob/master/detection/scrfd)  | 16.9Mb |  

## Face Alignment.

<div id="lite.ai.toolkit.hub.tnn-face-alignment"></div>  


|             Class             | Pretrained TNN Files |               Rename or Converted From (Repo)                | Size  |
| :---------------------------: | :-------------------: | :----------------------------------------------------------: | :---: |
| *lite::tnn::cv::face::align::PFLD* |  pfld-106-lite.opt.tnnproto&tnnmodel   | [pfld_106_...](https://github.com/Hsintao/pfld_106_face_landmarks) | 1.0Mb |
| *lite::tnn::cv::face::align::PFLD* |   pfld-106-v3.opt.tnnproto&tnnmodel    | [pfld_106_...](https://github.com/Hsintao/pfld_106_face_landmarks) | 5.5Mb |
| *lite::tnn::cv::face::align::PFLD* |   pfld-106-v2.opt.tnnproto&tnnmodel    | [pfld_106_...](https://github.com/Hsintao/pfld_106_face_landmarks) | 5.0Mb |
| *lite::tnn::cv::face::align::PFLD98* |   PFLD-pytorch-pfld.opt.tnnproto&tnnmodel  | [PFLD...](https://github.com/polarisZhao/PFLD-pytorch) | 4.8Mb |
| *lite::tnn::cv::face::align::MobileNetV268* |   pytorch_face_landmarks_landmark_detection_56.opt.tnnproto&tnnmodel  | [...landmark](https://github.com/cunjian/pytorch_face_landmark) | 9.4Mb |
| *lite::tnn::cv::face::align::MobileNetV2SE68* |   pytorch_face_landmarks_landmark_detection_56_se_external.opt.tnnproto&tnnmodel  | [...landmark](https://github.com/cunjian/pytorch_face_landmark) | 11Mb |
| *lite::tnn::cv::face::align::PFLD68* |   pytorch_face_landmarks_pfld.opt.tnnproto&tnnmodel  | [...landmark](https://github.com/cunjian/pytorch_face_landmark) | 2.8Mb |
| *lite::tnn::cv::face::align::FaceLandmarks1000* |   FaceLandmark1000.opt.tnnproto&tnnmodel  | [FaceLandm...](https://github.com/Single430/FaceLandmark1000) | 2.0Mb |

## Head Pose Estimation.

<div id="lite.ai.toolkit.hub.tnn-head-pose-estimation"></div>  


|             Class              | Pretrained TNN Files |               Rename or Converted From (Repo)                | Size  |
| :----------------------------: | :-------------------: | :----------------------------------------------------------: | :---: |
| *lite::tnn::cv::face::pose::FSANet* |    fsanet-var.opt.tnnproto&tnnmodel    | [...fsanet...](https://github.com/omasaht/headpose-fsanet-pytorch) | 1.2Mb |
| *lite::tnn::cv::face::pose::FSANet* |    fsanet-1x1.opt.tnnproto&tnnmodel    | [...fsanet...](https://github.com/omasaht/headpose-fsanet-pytorch) | 1.2Mb |

## Face Attributes.

<div id="lite.ai.toolkit.hub.tnn-face-attributes"></div>  


|                  Class                  |          Pretrained TNN Files           |             Rename or Converted From (Repo)              | Size  |
| :-------------------------------------: | :--------------------------------------: | :------------------------------------------------------: | :---: |
|  *lite::tnn::cv::face::attr::AgeGoogleNet*   |            age_googlenet.opt.tnnproto&tnnmodel            |      [onnx-models](https://github.com/onnx/models)       | 23Mb  |
| *lite::tnn::cv::face::attr::GenderGoogleNet* |          gender_googlenet.opt.tnnproto&tnnmodel           |      [onnx-models](https://github.com/onnx/models)       | 23Mb  |
| *lite::tnn::cv::face::attr::EmotionFerPlus*  |          emotion-ferplus-7.opt.tnnproto&tnnmodel          |      [onnx-models](https://github.com/onnx/models)       | 33Mb  |
| *lite::tnn::cv::face::attr::EmotionFerPlus*  |          emotion-ferplus-8.opt.tnnproto&tnnmodel          |      [onnx-models](https://github.com/onnx/models)       | 33Mb  |
|     *lite::tnn::cv::face::attr::SSRNet*      |               ssrnet.opt.tnnproto&tnnmodel                | [SSR_Net...](https://github.com/oukohou/SSR_Net_Pytorch) | 190Kb |
|     *lite::tnn::cv::face::attr::EfficientEmotion7*      |  face-emotion-recognition-enet_b0_7.opt.tnnproto&tnnmodel  | [face-emo...](https://github.com/HSE-asavchenko/face-emotion-recognition) | 15Mb |
|     *lite::tnn::cv::face::attr::EfficientEmotion8*      |  face-emotion-recognition-enet_b0_8_best_afew.opt.tnnproto&tnnmodel | [face-emo...](https://github.com/HSE-asavchenko/face-emotion-recognition) | 15Mb |
|     *lite::tnn::cv::face::attr::EfficientEmotion8*      |  face-emotion-recognition-enet_b0_8_best_vgaf.opt.tnnproto&tnnmodel | [face-emo...](https://github.com/HSE-asavchenko/face-emotion-recognition) | 15Mb |
|     *lite::tnn::cv::face::attr::MobileEmotion7*      |   face-emotion-recognition-mobilenet_7.opt.tnnproto&tnnmodel  | [face-emo...](https://github.com/HSE-asavchenko/face-emotion-recognition)| 13Mb |
|     *lite::tnn::cv::face::attr::ReXNetEmotion7*      | face-emotion-recognition-affectnet_7_vggface2_rexnet150.opt.tnnproto&tnnmodel | [face-emo...](https://github.com/HSE-asavchenko/face-emotion-recognition) | 30Mb |

## Classification.

<div id="lite.ai.toolkit.hub.tnn-classification"></div>


|                    Class                     |   Pretrained TNN Files    |         Rename or Converted From (Repo)          | Size  |
| :------------------------------------------: | :------------------------: | :----------------------------------------------: | :---: |
| *lite::tnn::cv::classification:EfficientNetLite4* | efficientnet-lite4-11.opt.tnnproto&tnnmodel |  [onnx-models](https://github.com/onnx/models)   | 49Mb  |
|   *lite::tnn::cv::classification::ShuffleNetV2*   |   shufflenet-v2-10.opt.tnnproto&tnnmodel    |  [onnx-models](https://github.com/onnx/models)   | 8.7Mb |
|   *lite::tnn::cv::classification::DenseNet121*    |      densenet121.opt.tnnproto&tnnmodel      | [torchvision](https://github.com/pytorch/vision) | 30Mb  |
|     *lite::tnn::cv::classification::GhostNet*     |       ghostnet.opt.tnnproto&tnnmodel        | [torchvision](https://github.com/pytorch/vision) | 20Mb  |
|     *lite::tnn::cv::classification::HdrDNet*      |        hardnet.opt.tnnproto&tnnmodel        | [torchvision](https://github.com/pytorch/vision) | 13Mb  |
|      *lite::tnn::cv::classification::IBNNet*      |       ibnnet18.opt.tnnproto&tnnmodel        | [torchvision](https://github.com/pytorch/vision) | 97Mb  |
|   *lite::tnn::cv::classification::MobileNetV2*    |      mobilenetv2.opt.tnnproto&tnnmodel      | [torchvision](https://github.com/pytorch/vision) | 13Mb  |
|      *lite::tnn::cv::classification::ResNet*      |       resnet18.opt.tnnproto&tnnmodel        | [torchvision](https://github.com/pytorch/vision) | 44Mb  |
|     *lite::tnn::cv::classification::ResNeXt*      |        resnext.opt.tnnproto&tnnmodel        | [torchvision](https://github.com/pytorch/vision) | 95Mb  |


## Segmentation.

<div id="lite.ai.toolkit.hub.onnx-segmentation"></div>  


|                    Class                     |     Pretrained TNN Files     |         Rename or Converted From (Repo)          | Size  |
| :------------------------------------------: | :---------------------------: | :----------------------------------------------: | :---: |
| *lite::cv::segmentation::DeepLabV3ResNet101* | deeplabv3_resnet101_coco.opt.tnnproto&tnnmodel | [torchvision](https://github.com/pytorch/vision) | 232Mb |
|    *lite::cv::segmentation::FCNResNet101*    |      fcn_resnet101.opt.tnnproto&tnnmodel       | [torchvision](https://github.com/pytorch/vision) | 207Mb |


## Style Transfer.

<div id="lite.ai.toolkit.hub.tnn-style-transfer"></div>  

|                Class                 |   Pretrained TNN Files    |        Rename or Converted From (Repo)        | Size  |
| :----------------------------------: | :------------------------: | :-------------------------------------------: | :---: |
| *lite::tnn::cv::style::FastStyleTransfer* |    style-mosaic-8.opt.tnnproto&tnnmodel     | [onnx-models](https://github.com/onnx/models) | 6.4Mb |
| *lite::tnn::cv::style::FastStyleTransfer* |     style-candy-9.opt.tnnproto&tnnmodel     | [onnx-models](https://github.com/onnx/models) | 6.4Mb |
| *lite::tnn::cv::style::FastStyleTransfer* |     style-udnie-8.opt.tnnproto&tnnmodel     | [onnx-models](https://github.com/onnx/models) | 6.4Mb |
| *lite::tnn::cv::style::FastStyleTransfer* |     style-udnie-9.opt.tnnproto&tnnmodel     | [onnx-models](https://github.com/onnx/models) | 6.4Mb |
| *lite::tnn::cv::style::FastStyleTransfer* |  style-pointilism-8.opt.tnnproto&tnnmodel   | [onnx-models](https://github.com/onnx/models) | 6.4Mb |
| *lite::tnn::cv::style::FastStyleTransfer* |  style-pointilism-9.opt.tnnproto&tnnmodel   | [onnx-models](https://github.com/onnx/models) | 6.4Mb |
| *lite::tnn::cv::style::FastStyleTransfer* | style-rain-princess-9.opt.tnnproto&tnnmodel | [onnx-models](https://github.com/onnx/models) | 6.4Mb |
| *lite::tnn::cv::style::FastStyleTransfer* | style-rain-princess-8.opt.tnnproto&tnnmodel | [onnx-models](https://github.com/onnx/models) | 6.4Mb |
| *lite::tnn::cv::style::FastStyleTransfer* |     style-candy-8.opt.tnnproto&tnnmodel     | [onnx-models](https://github.com/onnx/models) | 6.4Mb |
| *lite::tnn::cv::style::FastStyleTransfer* |    style-mosaic-9.opt.tnnproto&tnnmodel     | [onnx-models](https://github.com/onnx/models) | 6.4Mb |


## Colorization.

<div id="lite.ai.toolkit.hub.tnn-colorization"></div>

|                Class                |   Pretrained TNN Files   |              Rename or Converted From (Repo)              | Size  |
| :---------------------------------: | :-----------------------: | :-------------------------------------------------------: | :---: |
| *lite::tnn::cv::colorization::Colorizer* |   eccv16-colorizer.opt.tnnproto&tnnmodel   | [colorization](https://github.com/richzhang/colorization) | 123Mb |
| *lite::tnn::cv::colorization::Colorizer* | siggraph17-colorizer.opt.tnnproto&tnnmodel | [colorization](https://github.com/richzhang/colorization) | 129Mb |


## Super Resolution.

<div id="lite.ai.toolkit.hub.tnn-super-resolution"></div>

|                Class                | Pretrained TNN Files |              Rename or Converted From (Repo)              | Size  |
| :---------------------------------: | :-------------------: | :-------------------------------------------------------: | :---: |
| *lite::tnn::cv::resolution::SubPixelCNN* |   subpixel-cnn.opt.tnnproto&tnnmodel   | [...PIXEL...](https://github.com/niazwazir/SUB_PIXEL_CNN) | 234Kb |


#