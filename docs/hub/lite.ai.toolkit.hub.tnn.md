# Lite.AI.ToolKit.Hub.TNN

You can download all the pretrained models files of TNN format from ([Baidu Drive](https://pan.baidu.com/s/1lvM2YKyUbEc5HKVtqITpcw) code: 6o6k)

## Object Detection.

<div id="lite.ai.toolkit.hub.tnn-object-detection"></div>

|                 Class                 |      Pretrained TNN Files      |              Rename or Converted From (Repo)              | Size  |
| :-----------------------------------: | :-----------------------------: | :-------------------------------------------------------: | :---: |
|     *lite::tnn::cv::detection::YoloV5*     |          yolov5l.opt.tnnproto&tnnmodel            |      [yolov5](https://github.com/ultralytics/yolov5)      | 188Mb |
|     *lite::tnn::cv::detection::YoloV5*     |          yolov5m.opt.tnnproto&tnnmodel            |      [yolov5](https://github.com/ultralytics/yolov5)      | 85Mb  |
|     *lite::tnn::cv::detection::YoloV5*     |          yolov5s.opt.tnnproto&tnnmodel            |      [yolov5](https://github.com/ultralytics/yolov5)      | 29Mb  |
|     *lite::tnn::cv::detection::YoloV5*     |          yolov5x.opt.tnnproto&tnnmodel            |      [yolov5](https://github.com/ultralytics/yolov5)      | 351Mb |  
|     *lite::tnn::cv::detection::YoloX*      |          yolox_x.opt.tnnproto&tnnmodel           |  [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX)   | 378Mb |
|     *lite::tnn::cv::detection::YoloX*      |          yolox_l.opt.tnnproto&tnnmodel           |  [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX)   | 207Mb |
|     *lite::tnn::cv::detection::YoloX*      |          yolox_m.opt.tnnproto&tnnmodel           |  [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX)   | 97Mb  |
|     *lite::tnn::cv::detection::YoloX*      |          yolox_s.opt.tnnproto&tnnmodel           |  [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX)   | 34Mb  |
|     *lite::tnn::cv::detection::YoloX*      |         yolox_tiny.opt.tnnproto&tnnmodel         |  [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX)   | 19Mb  |
|     *lite::tnn::cv::detection::YoloX*      |         yolox_nano.opt.tnnproto&tnnmodel         |  [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX)   | 3.5Mb |
|     *lite::tnn::cv::detection::YOLOP*      |          yolop-320-320.opt.tnnproto&tnnmodel           |  [YOLOP](https://github.com/hustvl/YOLOP)   | 30Mb |
|     *lite::tnn::cv::detection::YOLOP*      |          yolop-640-640.opt.tnnproto&tnnmodel           |  [YOLOP](https://github.com/hustvl/YOLOP)   | 30Mb |
|     *lite::tnn::cv::detection::YOLOP*      |          yolop-1280-1280.opt.tnnproto&tnnmodel           |  [YOLOP](https://github.com/hustvl/YOLOP)   | 30Mb  |
| *lite::tnn::cv::detection::NanoDet* |    nanodet_m_0.5x.opt.tnnproto&tnnmodel     |       [nanodet](https://github.com/RangiLyu/nanodet)       | 1.1Mb  |
| *lite::tnn::cv::detection::NanoDet* |    nanodet_m.opt.tnnproto&tnnmodel     |       [nanodet](https://github.com/RangiLyu/nanodet)       | 3.6Mb  |
| *lite::tnn::cv::detection::NanoDet* |    nanodet_m_1.5x.opt.tnnproto&tnnmodel     |       [nanodet](https://github.com/RangiLyu/nanodet)       | 7.9Mb  |
| *lite::tnn::cv::detection::NanoDet* |    nanodet_m_1.5x_416.opt.tnnproto&tnnmodel     |       [nanodet](https://github.com/RangiLyu/nanodet)       | 7.9Mb  |
| *lite::tnn::cv::detection::NanoDet* |    nanodet_m_416.opt.tnnproto&tnnmodel     |       [nanodet](https://github.com/RangiLyu/nanodet)       | 3.6Mb  |
| *lite::tnn::cv::detection::NanoDet* |    nanodet_g.opt.tnnproto&tnnmodel     |       [nanodet](https://github.com/RangiLyu/nanodet)       | 14Mb  |
| *lite::tnn::cv::detection::NanoDet* |    nanodet_t.opt.tnnproto&tnnmodel     |       [nanodet](https://github.com/RangiLyu/nanodet)       | 5.1Mb  |
| *lite::tnn::cv::detection::NanoDet* |    nanodet-RepVGG-A0_416.opt.tnnproto&tnnmodel     |       [nanodet](https://github.com/RangiLyu/nanodet)       | 26Mb  |
| *lite::tnn::cv::detection::NanoDetEfficientNetLite* |    nanodet-EfficientNet-Lite0_320.opt.tnnproto&tnnmodel     |       [nanodet](https://github.com/RangiLyu/nanodet)       | 12Mb  |
| *lite::tnn::cv::detection::NanoDetEfficientNetLite* |    nanodet-EfficientNet-Lite1_416.opt.tnnproto&tnnmodel     |       [nanodet](https://github.com/RangiLyu/nanodet)       | 15Mb  |
| *lite::tnn::cv::detection::NanoDetEfficientNetLite* |    nanodet-EfficientNet-Lite2_512.opt.tnnproto&tnnmodel     |       [nanodet](https://github.com/RangiLyu/nanodet)       | 18Mb  |
|     *lite::tnn::cv::detection::YoloX_V_0_1_1*      |          yolox_x_v0.1.1.opt.tnnproto&tnnmodel           |  [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX)   | 378Mb |
|     *lite::tnn::cv::detection::YoloX_V_0_1_1*      |          yolox_l_v0.1.1.opt.tnnproto&tnnmodel           |  [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX)   | 207Mb |
|     *lite::tnn::cv::detection::YoloX_V_0_1_1*      |          yolox_m_v0.1.1.opt.tnnproto&tnnmodel           |  [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX)   | 97Mb  |
|     *lite::tnn::cv::detection::YoloX_V_0_1_1*      |          yolox_s_v0.1.1.opt.tnnproto&tnnmodel           |  [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX)   | 34Mb  |
|     *lite::tnn::cv::detection::YoloX_V_0_1_1*      |         yolox_tiny_v0.1.1.opt.tnnproto&tnnmodel         |  [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX)   | 19Mb  |
|     *lite::tnn::cv::detection::YoloX_V_0_1_1*      |         yolox_nano_v0.1.1.opt.tnnproto&tnnmodel         |  [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX)   | 3.5Mb |
|     *lite::tnn::cv::detection::YoloR*     |          yolor-p6-320-320.opt.tnnproto&tnnmodel            |      [yolor](https://github.com/WongKinYiu/yolor)      | 157Mb |
|     *lite::tnn::cv::detection::YoloR*     |          yolor-p6-640-640.opt.tnnproto&tnnmodel            |      [yolor](https://github.com/WongKinYiu/yolor)      | 157Mb  |
|     *lite::tnn::cv::detection::YoloR*     |          yolor-ssss-s2d-640-640.opt.tnnproto&tnnmodel            |      [yolor](https://github.com/WongKinYiu/yolor)      | 50Mb  |
|     *lite::tnn::cv::detection::YoloV5_V_6_0*     |          yolov5m.640-640.v.6.0.opt.tnnproto&tnnmodel       |      [yolov5](https://github.com/ultralytics/yolov5)      | 81Mb  |
|     *lite::tnn::cv::detection::YoloV5_V_6_0*     |          yolov5s.640-640.v.6.0.opt.tnnproto&tnnmodel           |      [yolov5](https://github.com/ultralytics/yolov5)      | 28Mb  |
|     *lite::tnn::cv::detection::YoloV5_V_6_0*     |          yolov5n.640-640.v.6.0.opt.tnnproto&tnnmodel           |      [yolov5](https://github.com/ultralytics/yolov5)      | 7.5Mb |
|     *lite::tnn::cv::detection::YoloV5_V_6_0*     |          yolov5m6.640-640.v.6.0.opt.tnnproto&tnnmodel           |      [yolov5](https://github.com/ultralytics/yolov5)      | 128Mb  |
|     *lite::tnn::cv::detection::YoloV5_V_6_0*     |          yolov5s6.640-640.v.6.0.opt.tnnproto&tnnmodel           |      [yolov5](https://github.com/ultralytics/yolov5)      | 50Mb  |
|     *lite::tnn::cv::detection::YoloV5_V_6_0*     |          yolov5n6.640-640.v.6.0.opt.tnnproto&tnnmodel           |      [yolov5](https://github.com/ultralytics/yolov5)      | 14Mb |
|     *lite::tnn::cv::detection::YoloV5_V_6_0*     |          yolov5m6.1280-1280.v.6.0.opt.tnnproto&tnnmodel           |      [yolov5](https://github.com/ultralytics/yolov5)      | 128Mb  |
|     *lite::tnn::cv::detection::YoloV5_V_6_0*     |          yolov5s6.1280-1280.v.6.0.opt.tnnproto&tnnmodel           |      [yolov5](https://github.com/ultralytics/yolov5)      | 50Mb  |
|     *lite::tnn::cv::detection::YoloV5_V_6_0*     |          yolov5n6.1280-1280.v.6.0.opt.tnnproto&tnnmodel           |      [yolov5](https://github.com/ultralytics/yolov5)      | 14Mb |


## Matting.

<div id="lite.ai.toolkit.hub.tnn-matting"></div>

|                Class                | Pretrained TNN Files |              Rename or Converted From (Repo)              | Size  |
| :---------------------------------: | :-------------------: | :-------------------------------------------------------: | :---: |
| *lite::tnn::cv::matting::RobustVideoMatting* |   rvm_mobilenetv3_fp32-480-480-sim.tnnproto&tnnmodel   | [RobustVideoMatting](https://github.com/PeterL1n/RobustVideoMatting) | 14Mb |
| *lite::tnn::cv::matting::RobustVideoMatting* |   rvm_mobilenetv3_fp32-480-480-sim.opt.tnnproto&tnnmodel   | [RobustVideoMatting](https://github.com/PeterL1n/RobustVideoMatting) | 14Mb |
| *lite::tnn::cv::matting::RobustVideoMatting* |   rvm_mobilenetv3_fp32-480-640-sim.opt.tnnproto&tnnmodel   | [RobustVideoMatting](https://github.com/PeterL1n/RobustVideoMatting) | 14Mb |
| *lite::tnn::cv::matting::RobustVideoMatting* |   rvm_mobilenetv3_fp32-640-480-sim.opt.tnnproto&tnnmodel   | [RobustVideoMatting](https://github.com/PeterL1n/RobustVideoMatting) | 14Mb |
| *lite::tnn::cv::matting::RobustVideoMatting* |   rvm_mobilenetv3_fp32-1080-1920-sim.opt.tnnproto&tnnmodel   | [RobustVideoMatting](https://github.com/PeterL1n/RobustVideoMatting) | 14Mb |
| *lite::tnn::cv::matting::RobustVideoMatting* |   rvm_resnet50_fp32-480-480-sim.opt.tnnproto&tnnmodel   | [RobustVideoMatting](https://github.com/PeterL1n/RobustVideoMatting) | 50Mb |
| *lite::tnn::cv::matting::RobustVideoMatting* |   rvm_resnet50_fp32-480-640-sim.opt.tnnproto&tnnmodel   | [RobustVideoMatting](https://github.com/PeterL1n/RobustVideoMatting) | 50Mb |
| *lite::tnn::cv::matting::RobustVideoMatting* |   rvm_resnet50_fp32-640-480-sim.opt.tnnproto&tnnmodel   | [RobustVideoMatting](https://github.com/PeterL1n/RobustVideoMatting) | 50Mb |
| *lite::tnn::cv::matting::RobustVideoMatting* |   rvm_resnet50_fp32-1080-1920-sim.opt.tnnproto&tnnmodel   | [RobustVideoMatting](https://github.com/PeterL1n/RobustVideoMatting) | 50Mb |

## Face Recognition.

<div id="lite.ai.toolkit.hub.tnn-face-recognition"></div>  


|                   Class                   |            Pretrained TNN Files             |               Rename or Converted From (Repo)                | Size  |
| :---------------------------------------: | :------------------------------------------: | :----------------------------------------------------------: | :---: |
|     *lite::tnn::cv::faceid::GlintArcFace*      |           ms1mv3_arcface_r100.opt.tnnproto&tnnmodel           |  [insightface](https://github.com/deepinsight/insightface)   | 248Mb |
|     *lite::tnn::cv::faceid::GlintArcFace*      |           ms1mv3_arcface_r50.opt.tnnproto&tnnmodel            |  [insightface](https://github.com/deepinsight/insightface)   | 166Mb |
|     *lite::tnn::cv::faceid::GlintArcFace*      |           ms1mv3_arcface_r34.opt.tnnproto&tnnmodel            |  [insightface](https://github.com/deepinsight/insightface)   | 130Mb |
|     *lite::tnn::cv::faceid::GlintArcFace*      |           ms1mv3_arcface_r18.opt.tnnproto&tnnmodel            |  [insightface](https://github.com/deepinsight/insightface)   | 91Mb  |
|     *lite::tnn::cv::faceid::GlintCosFace*      |         glint360k_cosface_r100.opt.tnnproto&tnnmodel          |  [insightface](https://github.com/deepinsight/insightface)   | 248Mb |
|     *lite::tnn::cv::faceid::GlintCosFace*      |          glint360k_cosface_r50.opt.tnnproto&tnnmodel          |  [insightface](https://github.com/deepinsight/insightface)   | 166Mb |
|     *lite::tnn::cv::faceid::GlintCosFace*      |          glint360k_cosface_r34.opt.tnnproto&tnnmodel          |  [insightface](https://github.com/deepinsight/insightface)   | 130Mb |
|     *lite::tnn::cv::faceid::GlintCosFace*      |          glint360k_cosface_r18.opt.tnnproto&tnnmodel          |  [insightface](https://github.com/deepinsight/insightface)   | 91Mb  |
|    *lite::tnn::cv::faceid::GlintPartialFC*     |        partial_fc_glint360k_r100.opt.tnnproto&tnnmodel        |  [insightface](https://github.com/deepinsight/insightface)   | 248Mb |
|    *lite::tnn::cv::faceid::GlintPartialFC*     |        partial_fc_glint360k_r50.opt.tnnproto&tnnmodel         |  [insightface](https://github.com/deepinsight/insightface)   | 91Mb  |
|        *lite::tnn::cv::faceid::FaceNet*        |         facenet_vggface2_resnet.opt.tnnproto&tnnmodel         |  [facenet...](https://github.com/timesler/facenet-pytorch)   | 89Mb  |
|        *lite::tnn::cv::faceid::FaceNet*        |      facenet_casia-webface_resnet.opt.tnnproto&tnnmodel       |  [facenet...](https://github.com/timesler/facenet-pytorch)   | 89Mb  |
|     *lite::tnn::cv::faceid::FocalArcFace*      |        focal-arcface-ms1m-ir152.opt.tnnproto&tnnmodel         | [face.evoLVe...](https://github.com/ZhaoJ9014/face.evoLVe.PyTorch) | 269Mb |
|     *lite::tnn::cv::faceid::FocalArcFace*      |    focal-arcface-ms1m-ir50-epoch120.opt.tnnproto&tnnmodel     | [face.evoLVe...](https://github.com/ZhaoJ9014/face.evoLVe.PyTorch) | 166Mb |
|     *lite::tnn::cv::faceid::FocalArcFace*      |     focal-arcface-ms1m-ir50-epoch63.opt.tnnproto&tnnmodel     | [face.evoLVe...](https://github.com/ZhaoJ9014/face.evoLVe.PyTorch) | 166Mb |
|   *lite::tnn::cv::faceid::FocalAsiaArcFace*    |       focal-arcface-bh-ir50-asia.opt.tnnproto&tnnmodel        | [face.evoLVe...](https://github.com/ZhaoJ9014/face.evoLVe.PyTorch) | 166Mb |
| *lite::tnn::cv::faceid::TencentCurricularFace* |     Tencent_CurricularFace_Backbone.opt.tnnproto&tnnmodel     |          [TFace](https://github.com/Tencent/TFace)           | 249Mb |
|    *lite::tnn::cv::faceid::TencentCifpFace*    |  Tencent_Cifp_BUPT_Balancedface_IR_34.opt.tnnproto&tnnmodel   |          [TFace](https://github.com/Tencent/TFace)           | 130Mb |
|    *lite::tnn::cv::faceid::CenterLossFace*     |        CenterLossFace_epoch_100.opt.tnnproto&tnnmodel         | [center-loss...](https://github.com/louis-she/center-loss.pytorch) | 280Mb |
|      *lite::tnn::cv::faceid::SphereFace*       |           sphere20a_20171020.opt.tnnproto&tnnmodel            | [sphere...](https://github.com/clcarwin/sphereface_pytorch)  | 86Mb  |
|     *lite::tnn::cv::faceid:MobileFaceNet*      |        MobileFaceNet_Pytorch_068.opt.tnnproto&tnnmodel        | [MobileFace...](https://github.com/Xiaoccer/MobileFaceNet_Pytorch) | 3.8Mb |
|    *lite::tnn::cv::faceid:CavaGhostArcFace*    | cavaface_GhostNet_x1.3_Arcface_Epoch_24.opt.tnnproto&tnnmodel | [cavaface...](https://github.com/cavalleria/cavaface.pytorch) | 15Mb  |
|    *lite::tnn::cv::faceid:CavaCombinedFace*    |  cavaface_IR_SE_100_Combined_Epoch_24.opt.tnnproto&tnnmodel   | [cavaface...](https://github.com/cavalleria/cavaface.pytorch) | 250Mb |
|    *lite::tnn::cv::faceid:MobileSEFocalFace*   | face_recognition.pytorch_Mobilenet_se_focal_121000.opt.tnnproto&tnnmodel | [face_recog...](https://github.com/grib0ed0v/face_recognition.pytorch) | 4.5Mb |

