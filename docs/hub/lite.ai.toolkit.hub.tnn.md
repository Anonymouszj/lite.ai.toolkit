# Lite.AI.ToolKit.Hub.TNN

You can download all the pretrained models files of TNN format from ([Baidu Drive](https://pan.baidu.com/s/1lvM2YKyUbEc5HKVtqITpcw) code: 6o6k)

## Object Detection.

<div id="lite.ai.toolkit.hub.tnn-object-detection"></div>

|                 Class                 |      Pretrained TNN Files      |              Rename or Converted From (Repo)              | Size  |
| :-----------------------------------: | :-----------------------------: | :-------------------------------------------------------: | :---: |
|     *lite::tnn::cv::detection::YoloV5*     |          yolov5l.opt.tnnproto&tnnmodel            |      [yolov5](https://github.com/ultralytics/yolov5)      | 188Mb |
|     *lite::tnn::cv::detection::YoloV5*     |          yolov5m.opt.tnnproto&tnnmodel            |      [yolov5](https://github.com/ultralytics/yolov5)      | 85Mb  |
|     *lite::tnn::cv::detection::YoloV5*     |          yolov5s.opt.tnnproto&tnnmodel            |      [yolov5](https://github.com/ultralytics/yolov5)      | 29Mb  |
|     *lite::tnn::cv::detection::YoloV5*     |          yolov5x.opt.tnnproto&tnnmodel            |      [yolov5](https://github.com/ultralytics/yolov5)      | 351Mb |  
|     *lite::tnn::cv::detection::YoloX*      |          yolox_x.opt.tnnproto&tnnmodel           |  [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX)   | 378Mb |
|     *lite::tnn::cv::detection::YoloX*      |          yolox_l.opt.tnnproto&tnnmodel           |  [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX)   | 207Mb |
|     *lite::tnn::cv::detection::YoloX*      |          yolox_m.opt.tnnproto&tnnmodel           |  [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX)   | 97Mb  |
|     *lite::tnn::cv::detection::YoloX*      |          yolox_s.opt.tnnproto&tnnmodel           |  [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX)   | 34Mb  |
|     *lite::tnn::cv::detection::YoloX*      |         yolox_tiny.opt.tnnproto&tnnmodel         |  [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX)   | 19Mb  |
|     *lite::tnn::cv::detection::YoloX*      |         yolox_nano.opt.tnnproto&tnnmodel         |  [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX)   | 3.5Mb |
|     *lite::tnn::cv::detection::YOLOP*      |          yolop-320-320.opt.tnnproto&tnnmodel           |  [YOLOP](https://github.com/hustvl/YOLOP)   | 30Mb |
|     *lite::tnn::cv::detection::YOLOP*      |          yolop-640-640.opt.tnnproto&tnnmodel           |  [YOLOP](https://github.com/hustvl/YOLOP)   | 30Mb |
|     *lite::tnn::cv::detection::YOLOP*      |          yolop-1280-1280.opt.tnnproto&tnnmodel           |  [YOLOP](https://github.com/hustvl/YOLOP)   | 30Mb  |
| *lite::tnn::cv::detection::NanoDet* |    nanodet_m_0.5x.opt.tnnproto&tnnmodel     |       [nanodet](https://github.com/RangiLyu/nanodet)       | 1.1Mb  |
| *lite::tnn::cv::detection::NanoDet* |    nanodet_m.opt.tnnproto&tnnmodel     |       [nanodet](https://github.com/RangiLyu/nanodet)       | 3.6Mb  |
| *lite::tnn::cv::detection::NanoDet* |    nanodet_m_1.5x.opt.tnnproto&tnnmodel     |       [nanodet](https://github.com/RangiLyu/nanodet)       | 7.9Mb  |
| *lite::tnn::cv::detection::NanoDet* |    nanodet_m_1.5x_416.opt.tnnproto&tnnmodel     |       [nanodet](https://github.com/RangiLyu/nanodet)       | 7.9Mb  |
| *lite::tnn::cv::detection::NanoDet* |    nanodet_m_416.opt.tnnproto&tnnmodel     |       [nanodet](https://github.com/RangiLyu/nanodet)       | 3.6Mb  |
| *lite::tnn::cv::detection::NanoDet* |    nanodet_g.opt.tnnproto&tnnmodel     |       [nanodet](https://github.com/RangiLyu/nanodet)       | 14Mb  |
| *lite::tnn::cv::detection::NanoDet* |    nanodet_t.opt.tnnproto&tnnmodel     |       [nanodet](https://github.com/RangiLyu/nanodet)       | 5.1Mb  |
| *lite::tnn::cv::detection::NanoDet* |    nanodet-RepVGG-A0_416.opt.tnnproto&tnnmodel     |       [nanodet](https://github.com/RangiLyu/nanodet)       | 26Mb  |
| *lite::tnn::cv::detection::NanoDetEfficientNetLite* |    nanodet-EfficientNet-Lite0_320.opt.tnnproto&tnnmodel     |       [nanodet](https://github.com/RangiLyu/nanodet)       | 12Mb  |
| *lite::tnn::cv::detection::NanoDetEfficientNetLite* |    nanodet-EfficientNet-Lite1_416.opt.tnnproto&tnnmodel     |       [nanodet](https://github.com/RangiLyu/nanodet)       | 15Mb  |
| *lite::tnn::cv::detection::NanoDetEfficientNetLite* |    nanodet-EfficientNet-Lite2_512.opt.tnnproto&tnnmodel     |       [nanodet](https://github.com/RangiLyu/nanodet)       | 18Mb  |
|     *lite::tnn::cv::detection::YoloX_V_0_1_1*      |          yolox_x_v0.1.1.opt.tnnproto&tnnmodel           |  [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX)   | 378Mb |
|     *lite::tnn::cv::detection::YoloX_V_0_1_1*      |          yolox_l_v0.1.1.opt.tnnproto&tnnmodel           |  [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX)   | 207Mb |
|     *lite::tnn::cv::detection::YoloX_V_0_1_1*      |          yolox_m_v0.1.1.opt.tnnproto&tnnmodel           |  [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX)   | 97Mb  |
|     *lite::tnn::cv::detection::YoloX_V_0_1_1*      |          yolox_s_v0.1.1.opt.tnnproto&tnnmodel           |  [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX)   | 34Mb  |
|     *lite::tnn::cv::detection::YoloX_V_0_1_1*      |         yolox_tiny_v0.1.1.opt.tnnproto&tnnmodel         |  [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX)   | 19Mb  |
|     *lite::tnn::cv::detection::YoloX_V_0_1_1*      |         yolox_nano_v0.1.1.opt.tnnproto&tnnmodel         |  [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX)   | 3.5Mb |
|     *lite::tnn::cv::detection::YoloR*     |          yolor-p6-320-320.opt.tnnproto&tnnmodel            |      [yolor](https://github.com/WongKinYiu/yolor)      | 157Mb |
|     *lite::tnn::cv::detection::YoloR*     |          yolor-p6-640-640.opt.tnnproto&tnnmodel            |      [yolor](https://github.com/WongKinYiu/yolor)      | 157Mb  |
|     *lite::tnn::cv::detection::YoloR*     |          yolor-ssss-s2d-640-640.opt.tnnproto&tnnmodel            |      [yolor](https://github.com/WongKinYiu/yolor)      | 50Mb  |
|     *lite::tnn::cv::detection::YoloV5_V_6_0*     |          yolov5m.640-640.v.6.0.opt.tnnproto&tnnmodel       |      [yolov5](https://github.com/ultralytics/yolov5)      | 81Mb  |
|     *lite::tnn::cv::detection::YoloV5_V_6_0*     |          yolov5s.640-640.v.6.0.opt.tnnproto&tnnmodel           |      [yolov5](https://github.com/ultralytics/yolov5)      | 28Mb  |
|     *lite::tnn::cv::detection::YoloV5_V_6_0*     |          yolov5n.640-640.v.6.0.opt.tnnproto&tnnmodel           |      [yolov5](https://github.com/ultralytics/yolov5)      | 7.5Mb |
|     *lite::tnn::cv::detection::YoloV5_V_6_0*     |          yolov5m6.640-640.v.6.0.opt.tnnproto&tnnmodel           |      [yolov5](https://github.com/ultralytics/yolov5)      | 128Mb  |
|     *lite::tnn::cv::detection::YoloV5_V_6_0*     |          yolov5s6.640-640.v.6.0.opt.tnnproto&tnnmodel           |      [yolov5](https://github.com/ultralytics/yolov5)      | 50Mb  |
|     *lite::tnn::cv::detection::YoloV5_V_6_0*     |          yolov5n6.640-640.v.6.0.opt.tnnproto&tnnmodel           |      [yolov5](https://github.com/ultralytics/yolov5)      | 14Mb |
|     *lite::tnn::cv::detection::YoloV5_V_6_0*     |          yolov5m6.1280-1280.v.6.0.opt.tnnproto&tnnmodel           |      [yolov5](https://github.com/ultralytics/yolov5)      | 128Mb  |
|     *lite::tnn::cv::detection::YoloV5_V_6_0*     |          yolov5s6.1280-1280.v.6.0.opt.tnnproto&tnnmodel           |      [yolov5](https://github.com/ultralytics/yolov5)      | 50Mb  |
|     *lite::tnn::cv::detection::YoloV5_V_6_0*     |          yolov5n6.1280-1280.v.6.0.opt.tnnproto&tnnmodel           |      [yolov5](https://github.com/ultralytics/yolov5)      | 14Mb |


## Matting.

<div id="lite.ai.toolkit.hub.tnn-matting"></div>

|                Class                | Pretrained TNN Files |              Rename or Converted From (Repo)              | Size  |
| :---------------------------------: | :-------------------: | :-------------------------------------------------------: | :---: |
| *lite::tnn::cv::matting::RobustVideoMatting* |   rvm_mobilenetv3_fp32-480-480-sim.tnnproto&tnnmodel   | [RobustVideoMatting](https://github.com/PeterL1n/RobustVideoMatting) | 14Mb |
| *lite::tnn::cv::matting::RobustVideoMatting* |   rvm_mobilenetv3_fp32-480-480-sim.opt.tnnproto&tnnmodel   | [RobustVideoMatting](https://github.com/PeterL1n/RobustVideoMatting) | 14Mb |
| *lite::tnn::cv::matting::RobustVideoMatting* |   rvm_mobilenetv3_fp32-480-640-sim.opt.tnnproto&tnnmodel   | [RobustVideoMatting](https://github.com/PeterL1n/RobustVideoMatting) | 14Mb |
| *lite::tnn::cv::matting::RobustVideoMatting* |   rvm_mobilenetv3_fp32-640-480-sim.opt.tnnproto&tnnmodel   | [RobustVideoMatting](https://github.com/PeterL1n/RobustVideoMatting) | 14Mb |
| *lite::tnn::cv::matting::RobustVideoMatting* |   rvm_mobilenetv3_fp32-1080-1920-sim.opt.tnnproto&tnnmodel   | [RobustVideoMatting](https://github.com/PeterL1n/RobustVideoMatting) | 14Mb |
| *lite::tnn::cv::matting::RobustVideoMatting* |   rvm_resnet50_fp32-480-480-sim.opt.tnnproto&tnnmodel   | [RobustVideoMatting](https://github.com/PeterL1n/RobustVideoMatting) | 50Mb |
| *lite::tnn::cv::matting::RobustVideoMatting* |   rvm_resnet50_fp32-480-640-sim.opt.tnnproto&tnnmodel   | [RobustVideoMatting](https://github.com/PeterL1n/RobustVideoMatting) | 50Mb |
| *lite::tnn::cv::matting::RobustVideoMatting* |   rvm_resnet50_fp32-640-480-sim.opt.tnnproto&tnnmodel   | [RobustVideoMatting](https://github.com/PeterL1n/RobustVideoMatting) | 50Mb |
| *lite::tnn::cv::matting::RobustVideoMatting* |   rvm_resnet50_fp32-1080-1920-sim.opt.tnnproto&tnnmodel   | [RobustVideoMatting](https://github.com/PeterL1n/RobustVideoMatting) | 50Mb |

## Face Recognition.

<div id="lite.ai.toolkit.hub.tnn-face-recognition"></div>  


|                   Class                   |            Pretrained TNN Files             |               Rename or Converted From (Repo)                | Size  |
| :---------------------------------------: | :------------------------------------------: | :----------------------------------------------------------: | :---: |
|     *lite::tnn::cv::faceid::GlintArcFace*      |           ms1mv3_arcface_r100.opt.tnnproto&tnnmodel           |  [insightface](https://github.com/deepinsight/insightface)   | 248Mb |
|     *lite::tnn::cv::faceid::GlintArcFace*      |           ms1mv3_arcface_r50.opt.tnnproto&tnnmodel            |  [insightface](https://github.com/deepinsight/insightface)   | 166Mb |
|     *lite::tnn::cv::faceid::GlintArcFace*      |           ms1mv3_arcface_r34.opt.tnnproto&tnnmodel            |  [insightface](https://github.com/deepinsight/insightface)   | 130Mb |
|     *lite::tnn::cv::faceid::GlintArcFace*      |           ms1mv3_arcface_r18.opt.tnnproto&tnnmodel            |  [insightface](https://github.com/deepinsight/insightface)   | 91Mb  |
|     *lite::tnn::cv::faceid::GlintCosFace*      |         glint360k_cosface_r100.opt.tnnproto&tnnmodel          |  [insightface](https://github.com/deepinsight/insightface)   | 248Mb |
|     *lite::tnn::cv::faceid::GlintCosFace*      |          glint360k_cosface_r50.opt.tnnproto&tnnmodel          |  [insightface](https://github.com/deepinsight/insightface)   | 166Mb |
|     *lite::tnn::cv::faceid::GlintCosFace*      |          glint360k_cosface_r34.opt.tnnproto&tnnmodel          |  [insightface](https://github.com/deepinsight/insightface)   | 130Mb |
|     *lite::tnn::cv::faceid::GlintCosFace*      |          glint360k_cosface_r18.opt.tnnproto&tnnmodel          |  [insightface](https://github.com/deepinsight/insightface)   | 91Mb  |
|    *lite::tnn::cv::faceid::GlintPartialFC*     |        partial_fc_glint360k_r100.opt.tnnproto&tnnmodel        |  [insightface](https://github.com/deepinsight/insightface)   | 248Mb |
|    *lite::tnn::cv::faceid::GlintPartialFC*     |        partial_fc_glint360k_r50.opt.tnnproto&tnnmodel         |  [insightface](https://github.com/deepinsight/insightface)   | 91Mb  |
|        *lite::tnn::cv::faceid::FaceNet*        |         facenet_vggface2_resnet.opt.tnnproto&tnnmodel         |  [facenet...](https://github.com/timesler/facenet-pytorch)   | 89Mb  |
|        *lite::tnn::cv::faceid::FaceNet*        |      facenet_casia-webface_resnet.opt.tnnproto&tnnmodel       |  [facenet...](https://github.com/timesler/facenet-pytorch)   | 89Mb  |
|     *lite::tnn::cv::faceid::FocalArcFace*      |        focal-arcface-ms1m-ir152.opt.tnnproto&tnnmodel         | [face.evoLVe...](https://github.com/ZhaoJ9014/face.evoLVe.PyTorch) | 269Mb |
|     *lite::tnn::cv::faceid::FocalArcFace*      |    focal-arcface-ms1m-ir50-epoch120.opt.tnnproto&tnnmodel     | [face.evoLVe...](https://github.com/ZhaoJ9014/face.evoLVe.PyTorch) | 166Mb |
|     *lite::tnn::cv::faceid::FocalArcFace*      |     focal-arcface-ms1m-ir50-epoch63.opt.tnnproto&tnnmodel     | [face.evoLVe...](https://github.com/ZhaoJ9014/face.evoLVe.PyTorch) | 166Mb |
|   *lite::tnn::cv::faceid::FocalAsiaArcFace*    |       focal-arcface-bh-ir50-asia.opt.tnnproto&tnnmodel        | [face.evoLVe...](https://github.com/ZhaoJ9014/face.evoLVe.PyTorch) | 166Mb |
| *lite::tnn::cv::faceid::TencentCurricularFace* |     Tencent_CurricularFace_Backbone.opt.tnnproto&tnnmodel     |          [TFace](https://github.com/Tencent/TFace)           | 249Mb |
|    *lite::tnn::cv::faceid::TencentCifpFace*    |  Tencent_Cifp_BUPT_Balancedface_IR_34.opt.tnnproto&tnnmodel   |          [TFace](https://github.com/Tencent/TFace)           | 130Mb |
|    *lite::tnn::cv::faceid::CenterLossFace*     |        CenterLossFace_epoch_100.opt.tnnproto&tnnmodel         | [center-loss...](https://github.com/louis-she/center-loss.pytorch) | 280Mb |
|      *lite::tnn::cv::faceid::SphereFace*       |           sphere20a_20171020.opt.tnnproto&tnnmodel            | [sphere...](https://github.com/clcarwin/sphereface_pytorch)  | 86Mb  |
|     *lite::tnn::cv::faceid:MobileFaceNet*      |        MobileFaceNet_Pytorch_068.opt.tnnproto&tnnmodel        | [MobileFace...](https://github.com/Xiaoccer/MobileFaceNet_Pytorch) | 3.8Mb |
|    *lite::tnn::cv::faceid:CavaGhostArcFace*    | cavaface_GhostNet_x1.3_Arcface_Epoch_24.opt.tnnproto&tnnmodel | [cavaface...](https://github.com/cavalleria/cavaface.pytorch) | 15Mb  |
|    *lite::tnn::cv::faceid:CavaCombinedFace*    |  cavaface_IR_SE_100_Combined_Epoch_24.opt.tnnproto&tnnmodel   | [cavaface...](https://github.com/cavalleria/cavaface.pytorch) | 250Mb |
|    *lite::tnn::cv::faceid:MobileSEFocalFace*   | face_recognition.pytorch_Mobilenet_se_focal_121000.opt.tnnproto&tnnmodel | [face_recog...](https://github.com/grib0ed0v/face_recognition.pytorch) | 4.5Mb |

## Face Detection.

<div id="lite.ai.toolkit.hub.tnn-face-detection"></div>  

|                Class                | Pretrained TNN Files  |               Rename or Converted From (Repo)                | Size  |
| :---------------------------------: | :--------------------: | :----------------------------------------------------------: | :---: |
| *lite::tnn::cv::face::detect::UltraFace* | ultraface-rfb-320.opt.tnnproto&tnnmodel | [Ultra-Light...](https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB) | 1.5Mb |
| *lite::tnn::cv::face::detect::UltraFace* | ultraface-rfb-640.opt.tnnproto&tnnmodel | [Ultra-Light...](https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB) | 1.5Mb |
| *lite::tnn::cv::face::detect::UltraFace* | ultraface-slim-320.opt.tnnproto&tnnmodel | [Ultra-Light...](https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB) | 1.2Mb |
| *lite::tnn::cv::face::detect::UltraFace* | ultraface-slim-640.opt.tnnproto&tnnmodel | [Ultra-Light...](https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB) | 1.2Mb |
| *lite::tnn::cv::face::detect::RetinaFace* | Pytorch_RetinaFace_mobile0.25.opt.tnnproto&tnnmodel | [...Retinaface](https://github.com/biubug6/Pytorch_Retinaface) | 1.6Mb |
| *lite::tnn::cv::face::detect::RetinaFace* | Pytorch_RetinaFace_mobile0.25-640-640.opt.tnnproto&tnnmodel | [...Retinaface](https://github.com/biubug6/Pytorch_Retinaface) | 1.6Mb |
| *lite::tnn::cv::face::detect::RetinaFace* | Pytorch_RetinaFace_mobile0.25-320-320.opt.tnnproto&tnnmodel | [...Retinaface](https://github.com/biubug6/Pytorch_Retinaface) | 1.6Mb |
| *lite::tnn::cv::face::detect::RetinaFace* | Pytorch_RetinaFace_mobile0.25-720-1080.opt.tnnproto&tnnmodel | [...Retinaface](https://github.com/biubug6/Pytorch_Retinaface) | 1.6Mb |
| *lite::tnn::cv::face::detect::FaceBoxes* | FaceBoxes.opt.tnnproto&tnnmodel | [FaceBoxes](https://github.com/zisianw/FaceBoxes.PyTorch)  | 3.8Mb |
| *lite::tnn::cv::face::detect::FaceBoxes* | FaceBoxes-640-640.opt.tnnproto&tnnmodel | [FaceBoxes](https://github.com/zisianw/FaceBoxes.PyTorch)  | 3.8Mb |
| *lite::tnn::cv::face::detect::FaceBoxes* | FaceBoxes-320-320.opt.tnnproto&tnnmodel | [FaceBoxes](https://github.com/zisianw/FaceBoxes.PyTorch)  | 3.8Mb |
| *lite::tnn::cv::face::detect::FaceBoxes* | FaceBoxes-720-1080.opt.tnnproto&tnnmodel | [FaceBoxes](https://github.com/zisianw/FaceBoxes.PyTorch)  | 3.8Mb |

## Face Alignment.

<div id="lite.ai.toolkit.hub.tnn-face-alignment"></div>  


|             Class             | Pretrained TNN Files |               Rename or Converted From (Repo)                | Size  |
| :---------------------------: | :-------------------: | :----------------------------------------------------------: | :---: |
| *lite::tnn::cv::face::align::PFLD* |  pfld-106-lite.opt.tnnproto&tnnmodel   | [pfld_106_...](https://github.com/Hsintao/pfld_106_face_landmarks) | 1.0Mb |
| *lite::tnn::cv::face::align::PFLD* |   pfld-106-v3.opt.tnnproto&tnnmodel    | [pfld_106_...](https://github.com/Hsintao/pfld_106_face_landmarks) | 5.5Mb |
| *lite::tnn::cv::face::align::PFLD* |   pfld-106-v2.opt.tnnproto&tnnmodel    | [pfld_106_...](https://github.com/Hsintao/pfld_106_face_landmarks) | 5.0Mb |
| *lite::tnn::cv::face::align::PFLD98* |   PFLD-pytorch-pfld.opt.tnnproto&tnnmodel  | [PFLD...](https://github.com/polarisZhao/PFLD-pytorch) | 4.8Mb |
| *lite::tnn::cv::face::align::MobileNetV268* |   pytorch_face_landmarks_landmark_detection_56.opt.tnnproto&tnnmodel  | [...landmark](https://github.com/cunjian/pytorch_face_landmark) | 9.4Mb |
| *lite::tnn::cv::face::align::MobileNetV2SE68* |   pytorch_face_landmarks_landmark_detection_56_se_external.opt.tnnproto&tnnmodel  | [...landmark](https://github.com/cunjian/pytorch_face_landmark) | 11Mb |
| *lite::tnn::cv::face::align::PFLD68* |   pytorch_face_landmarks_pfld.opt.tnnproto&tnnmodel  | [...landmark](https://github.com/cunjian/pytorch_face_landmark) | 2.8Mb |
| *lite::tnn::cv::face::align::FaceLandmarks1000* |   FaceLandmark1000.opt.tnnproto&tnnmodel  | [FaceLandm...](https://github.com/Single430/FaceLandmark1000) | 2.0Mb |

## Head Pose Estimation.

<div id="lite.ai.toolkit.hub.tnn-head-pose-estimation"></div>  


|             Class              | Pretrained TNN Files |               Rename or Converted From (Repo)                | Size  |
| :----------------------------: | :-------------------: | :----------------------------------------------------------: | :---: |
| *lite::tnn::cv::face::pose::FSANet* |    fsanet-var.opt.tnnproto&tnnmodel    | [...fsanet...](https://github.com/omasaht/headpose-fsanet-pytorch) | 1.2Mb |
| *lite::tnn::cv::face::pose::FSANet* |    fsanet-1x1.opt.tnnproto&tnnmodel    | [...fsanet...](https://github.com/omasaht/headpose-fsanet-pytorch) | 1.2Mb |

## Face Attributes.

<div id="lite.ai.toolkit.hub.tnn-face-attributes"></div>  


|                  Class                  |          Pretrained TNN Files           |             Rename or Converted From (Repo)              | Size  |
| :-------------------------------------: | :--------------------------------------: | :------------------------------------------------------: | :---: |
|  *lite::tnn::cv::face::attr::AgeGoogleNet*   |            age_googlenet.opt.tnnproto&tnnmodel            |      [onnx-models](https://github.com/onnx/models)       | 23Mb  |
| *lite::tnn::cv::face::attr::GenderGoogleNet* |          gender_googlenet.opt.tnnproto&tnnmodel           |      [onnx-models](https://github.com/onnx/models)       | 23Mb  |
| *lite::tnn::cv::face::attr::EmotionFerPlus*  |          emotion-ferplus-7.opt.tnnproto&tnnmodel          |      [onnx-models](https://github.com/onnx/models)       | 33Mb  |
| *lite::tnn::cv::face::attr::EmotionFerPlus*  |          emotion-ferplus-8.opt.tnnproto&tnnmodel          |      [onnx-models](https://github.com/onnx/models)       | 33Mb  |
|     *lite::tnn::cv::face::attr::SSRNet*      |               ssrnet.opt.tnnproto&tnnmodel                | [SSR_Net...](https://github.com/oukohou/SSR_Net_Pytorch) | 190Kb |
|     *lite::tnn::cv::face::attr::EfficientEmotion7*      |  face-emotion-recognition-enet_b0_7.opt.tnnproto&tnnmodel  | [face-emo...](https://github.com/HSE-asavchenko/face-emotion-recognition) | 15Mb |
|     *lite::tnn::cv::face::attr::EfficientEmotion8*      |  face-emotion-recognition-enet_b0_8_best_afew.opt.tnnproto&tnnmodel | [face-emo...](https://github.com/HSE-asavchenko/face-emotion-recognition) | 15Mb |
|     *lite::tnn::cv::face::attr::EfficientEmotion8*      |  face-emotion-recognition-enet_b0_8_best_vgaf.opt.tnnproto&tnnmodel | [face-emo...](https://github.com/HSE-asavchenko/face-emotion-recognition) | 15Mb |
|     *lite::tnn::cv::face::attr::MobileEmotion7*      |   face-emotion-recognition-mobilenet_7.opt.tnnproto&tnnmodel  | [face-emo...](https://github.com/HSE-asavchenko/face-emotion-recognition)| 13Mb |
|     *lite::tnn::cv::face::attr::ReXNetEmotion7*      | face-emotion-recognition-affectnet_7_vggface2_rexnet150.opt.tnnproto&tnnmodel | [face-emo...](https://github.com/HSE-asavchenko/face-emotion-recognition) | 30Mb |

